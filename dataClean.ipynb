{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataClean.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fEyv7E1nVk9T"
      },
      "source": [
        "# 1. Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7D8W2g8CVZ5U",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xeZl0KJRkXbA",
        "outputId": "b5a32489-c3a6-48b2-bd94-2a32acfb81f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c9A8p0cacxti",
        "colab": {},
        "outputId": "8a6d5bec-85cf-4fd3-cb4d-7e4546eaac08"
      },
      "source": [
        "os.chdir(\"D:\\\\Sensitive Data Detection\")\n",
        "ctr = 0\n",
        "file = open(r\"mergeddata1.csv\", encoding = \"ISO-8859-1\")\n",
        "total = len(file.readlines())\n",
        "print(total)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xMDs1F0nc-5W"
      },
      "source": [
        "# 2. Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LTkFvAHgdHe8"
      },
      "source": [
        "## a. Remove all non-ascii characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q4qQ2lqac-HM",
        "colab": {}
      },
      "source": [
        "def clean_nonascii(text):\n",
        "    return text.encode(\"ascii\",errors=\"ignore\").decode()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mIBvkQoTdY68"
      },
      "source": [
        "## b. Remove HTML(if any)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gFEjWZWqdX8q",
        "colab": {}
      },
      "source": [
        "def clean_html(text):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', text)\n",
        "    return cleantext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IOkYJKTzdj6y"
      },
      "source": [
        "## c. Replace different punctuation with whitespace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NPB3Pbegdi8I",
        "colab": {}
      },
      "source": [
        "def clean_punc(text):\n",
        "    return (text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KZKoqLFTdztD"
      },
      "source": [
        "## d. Case folding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "btQ-d7Zmdo-k",
        "colab": {}
      },
      "source": [
        "def clean_case(text):\n",
        "    return text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VFGx0WYZgJZ4"
      },
      "source": [
        "## e. Assuming digits don't provide any valuable insight in the dyncorp data - cleaning them\n",
        "*  For Banking Data, this should now be done. A better way of handling digits should be explored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4vuqBg_HgHg7",
        "colab": {}
      },
      "source": [
        "def clean_dig(text):\n",
        "    cleanr = re.compile('\\d')\n",
        "    cleantext = re.sub(cleanr, ' ', text)\n",
        "    return cleantext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2cU7eqDEgrPF"
      },
      "source": [
        "## f. Cleaning extra whitespaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u1mCjNcsggyU",
        "colab": {}
      },
      "source": [
        "def clean_ws(text):\n",
        "    return (' '.join(text.split()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-iNNy3pPgxSW"
      },
      "source": [
        "## g. Remove stopwords "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "59M4W62jgv82",
        "colab": {}
      },
      "source": [
        "def clean_stopwords(text):\n",
        "    setstopwords = set(stopwords.words('english'))\n",
        "    return \" \".join(word for word in text.split() if word not in setstopwords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ltKnjs3yiilT"
      },
      "source": [
        "## h. PoS Tagging \n",
        "* helper function for lemmatization to ensure verb and noun forms are treated differently"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GsL96rcSg33X",
        "colab": {}
      },
      "source": [
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\":wordnet.ADJ,\n",
        "                \"N\":wordnet.NOUN,\n",
        "                \"V\":wordnet.VERB,\n",
        "                \"R\":wordnet.ADV}\n",
        "    #default is noun\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FMi94YPeiz5A"
      },
      "source": [
        "## i. Lemmatize all sentences basis POS tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CtHa9LRDiw0t",
        "colab": {}
      },
      "source": [
        "def dolemma(text):\n",
        "    global ctr\n",
        "    global total\n",
        "#     print(ctr)\n",
        "    ctr = ctr + 1\n",
        "    sys.stdout.write('\\r')\n",
        "    sys.stdout.write('Processing %d/%d' % (ctr, total))\n",
        "    \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    word_list = nltk.word_tokenize(text)\n",
        "    lemmatized_output = \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in word_list])\n",
        "    return lemmatized_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BOJH0fbsjTty"
      },
      "source": [
        "## j. Combined clean routines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JRxTWU1fi4_c",
        "colab": {}
      },
      "source": [
        "def cleandf(df):\n",
        "    df['Text'] = df[['Text']].apply(lambda row: clean_case(row['Text']), axis=1)\n",
        "    df['Text'] = df[['Text']].apply(lambda row: clean_nonascii(str(row['Text'])), axis=1)\n",
        "    df['Text'] = df[['Text']].apply(lambda row: clean_html(row['Text']), axis=1)\n",
        "    df['Text'] = df[['Text']].apply(lambda row: clean_punc(row['Text']), axis=1)\n",
        "    df['Text'] = df[['Text']].apply(lambda row: clean_dig(row['Text']), axis=1)\n",
        "    df['Text'] = df[['Text']].apply(lambda row: clean_ws(row['Text']), axis=1)\n",
        "    df['Text'] = df[['Text']].apply(lambda row: clean_stopwords(row['Text']), axis=1)\n",
        "    df['Text'] = df[['Text']].apply(lambda row: dolemma(row['Text']), axis=1)\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ah5UayDqjhES"
      },
      "source": [
        "## k. Main Routine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b1W7V2OMjZFQ",
        "outputId": "58752c38-0d6b-4ddd-fdc4-b3d81f6499af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Step 1: Load the file into memory\n",
        "df = pd.read_csv(r\"mergeddata1.csv\",encoding = \"ISO-8859-1\")\n",
        "#Step 2: Clean the data\n",
        "df = cleandf(df)\n",
        "df.to_csv('cleaneddata.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing 10492/10637"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kY0Sb6ZhjtCO",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}